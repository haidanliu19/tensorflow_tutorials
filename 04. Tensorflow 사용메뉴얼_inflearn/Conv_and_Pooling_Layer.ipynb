{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv and Pooling Layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S6nPU7k8el6b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from termcolor import colored\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "\n",
        "from tensorflow.keras.metrics import Mean,SparseCategoricalAccuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_ds():\n",
        "    (train_validation_ds, test_ds) ,ds_info = tfds.load(name='mnist',\n",
        "                                                        shuffle_files=True,\n",
        "                                                        as_supervised=True,\n",
        "                                                        split=['train','test'],\n",
        "                                                               with_info=True)\n",
        "    n_train_validation = ds_info.splits['train'].num_examples\n",
        "\n",
        "    train_ratio = 0.8\n",
        "    n_train = int(n_train_validation * train_ratio)\n",
        "    n_validation = n_train_validation -n_train\n",
        "\n",
        "    train_ds = train_validation_ds.take(n_train)\n",
        "    remaning_ds = train_validation_ds.skip(n_train)\n",
        "    validation_ds = remaning_ds.take(n_validation)\n",
        "\n",
        "    return train_ds,validation_ds,test_ds\n",
        "\n",
        "train_ds,validation_ds,test_ds=get_mnist_ds()"
      ],
      "metadata": {
        "id": "7bHgoPQH6HSD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK8vTNkO7Aw7",
        "outputId": "444cd614-513e-4711-9cfe-2ba2d29f78b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardization(TRAIN_BATCH_SIZE,TEST_BATCH_SIZE):\n",
        "    global train_ds, validation_ds, test_ds\n",
        "\n",
        "    def stnd(images, labels):\n",
        "        images = tf.cast(images, tf.float32) /255.\n",
        "        return [images, labels]\n",
        "    \n",
        "    train_ds = train_ds.map(stnd).shuffle(1000).batch(TRAIN_BATCH_SIZE)\n",
        "    validation_ds = validation_ds.map(stnd).batch(TEST_BATCH_SIZE)\n",
        "    test_ds = test_ds.map(stnd).batch(TEST_BATCH_SIZE)\n",
        "\n"
      ],
      "metadata": {
        "id": "0cFAL-Mj7Gd9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_metrics():\n",
        "    global train_loss, train_acc\n",
        "    global validation_loss, validation_acc\n",
        "    global test_loss, test_acc\n",
        "\n",
        "    train_loss = Mean()\n",
        "    validation_loss = Mean()\n",
        "    test_loss = Mean()\n",
        "\n",
        "    train_acc = SparseCategoricalAccuracy()\n",
        "    validation_acc = SparseCategoricalAccuracy()\n",
        "    test_acc = SparseCategoricalAccuracy()"
      ],
      "metadata": {
        "id": "fO0E5wq98a9B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def trainer():\n",
        "    global train_ds, model, loss_object, optimizer\n",
        "    global train_loss, train_acc\n",
        "    for images, labels in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(images)\n",
        "            loss = loss_object(labels, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_acc(labels, predictions)"
      ],
      "metadata": {
        "id": "2Fa1E9mn8uUr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def validation():\n",
        "    global validation_ds, model, loss_object\n",
        "    global validation_loss, validation_acc\n",
        "    for images, labels in validation_ds:\n",
        "        predictions = model(images)\n",
        "        loss = loss_object(labels, predictions)\n",
        "\n",
        "        validation_loss(loss)\n",
        "        validation_acc(labels, predictions)"
      ],
      "metadata": {
        "id": "AKqM-zrL9a6W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def tester():\n",
        "    global test_ds, model, loss_object\n",
        "    global test_loss, test_acc\n",
        "    for images, labels in test_ds:\n",
        "        predictions = model(images)\n",
        "        loss = loss_object(labels, predictions)\n",
        "\n",
        "        test_loss(loss)\n",
        "        test_acc(labels, predictions)\n",
        "'''\n",
        "    template ='test Loss: {:.4f}\\t test accuracy:{:.2f}%\\n'\n",
        "\n",
        "    print(template.format(test_loss.result(),\n",
        "                          test_acc.result() * 100))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4LcqtNLL9pWl",
        "outputId": "944bc84c-84cc-49b3-c84e-1e802a1a86d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n    template ='test Loss: {:.4f}\\t test accuracy:{:.2f}%\\n'\\n\\n    print(template.format(test_loss.result(),\\n                          test_acc.result() * 100))\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reporter():\n",
        "    global epoch\n",
        "    global train_loss, train_acc\n",
        "    global validation_loss, validation_acc\n",
        "    #on \n",
        "    print(colored('Epoch','red','on_white'), epoch+1)\n",
        "\n",
        "    template = 'Train Loss: {:.4f}\\t Train Accuracy:{:.2f}%\\n' +\\\n",
        "    'validation Loss: {:.4f}\\t valiation accuracy:{:.2f}%\\n'\n",
        "\n",
        "    print(template.format(train_loss.result(),\n",
        "                          train_acc.result() * 100,\n",
        "                          validation_loss.result(),\n",
        "                          validation_acc.result() * 100))\n",
        "    train_acc.reset_states()\n",
        "    train_loss.reset_states()\n",
        "    validation_loss.reset_states()\n",
        "    validation_acc.reset_states()"
      ],
      "metadata": {
        "id": "6urWVid6_a2d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "TEST_BATCH_SIZE = 32\n",
        "\n",
        "standardization(TRAIN_BATCH_SIZE,TEST_BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "waLf0vnL9yD8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model = MNIST_Classfier()\n",
        "loss_object = SparseCategoricalCrossentropy()\n",
        "optimizer = SGD(learning_rate=LR)\n",
        "\n",
        "load_metrics()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    trainer()\n",
        "    validation()\n",
        "    train_reporter()\n",
        "\n",
        "tester()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "4A9SaRKq-XAg",
        "outputId": "6e40bb9f-dd2c-4ceb-fa69-d9ee956dc1f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = MNIST_Classfier()\\nloss_object = SparseCategoricalCrossentropy()\\noptimizer = SGD(learning_rate=LR)\\n\\nload_metrics()\\n\\nfor epoch in range(EPOCHS):\\n    trainer()\\n    validation()\\n    train_reporter()\\n\\ntester()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv = Conv2D(filters=8, kernel_size=3, padding='valid',activation='relu')\n",
        "pool = MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "image = tf.random.normal(mean=0, stddev = 1, shape=(1,28,28,1))\n",
        "print(image.shape)\n",
        "conved = conv(image)\n",
        "print(conved.shape)\n",
        "\n",
        "print('w: ',conv.get_weights()[0].shape)\n",
        "print('b: ',conv.get_weights()[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGOqntyO-von",
        "outputId": "5d9619e9-3875-4604-80a2-0bb1fd11e633"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 28, 28, 1)\n",
            "(1, 26, 26, 8)\n",
            "w:  (3, 3, 1, 8)\n",
            "b:  (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = pool(conved)\n",
        "print(pooled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IXuTpu-MBOD",
        "outputId": "a2bc81fe-b53a-4d37-bd33-fa1357f28169"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 13, 13, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "drRw5yuhMEpN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}