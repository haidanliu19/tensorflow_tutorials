{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S6nPU7k8el6b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from termcolor import colored\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Flatten,Dense,Activation\n",
        "\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "\n",
        "from tensorflow.keras.metrics import Mean,SparseCategoricalAccuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_ds():\n",
        "    (train_validation_ds, test_ds) ,ds_info = tfds.load(name='mnist',\n",
        "                                                        shuffle_files=True,\n",
        "                                                        as_supervised=True,\n",
        "                                                        split=['train','test'],\n",
        "                                                               with_info=True)\n",
        "    n_train_validation = ds_info.splits['train'].num_examples\n",
        "\n",
        "    train_ratio = 0.8\n",
        "    n_train = int(n_train_validation * train_ratio)\n",
        "    n_validation = n_train_validation -n_train\n",
        "\n",
        "    train_ds = train_validation_ds.take(n_train)\n",
        "    remaning_ds = train_validation_ds.skip(n_train)\n",
        "    validation_ds = remaning_ds.take(n_validation)\n",
        "\n",
        "    return train_ds,validation_ds,test_ds\n",
        "\n",
        "train_ds,validation_ds,test_ds=get_mnist_ds()"
      ],
      "metadata": {
        "id": "7bHgoPQH6HSD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK8vTNkO7Aw7",
        "outputId": "b3334ca9-566a-49df-8ebf-e99ce0eaa07a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardization(TRAIN_BATCH_SIZE,TEST_BATCH_SIZE):\n",
        "    global train_ds, validation_ds, test_ds\n",
        "\n",
        "    def stnd(images, labels):\n",
        "        images = tf.cast(images, tf.float32) /255.\n",
        "        return [images, labels]\n",
        "    \n",
        "    train_ds = train_ds.map(stnd).shuffle(1000).batch(TRAIN_BATCH_SIZE)\n",
        "    validation_ds = validation_ds.map(stnd).batch(TEST_BATCH_SIZE)\n",
        "    test_ds = test_ds.map(stnd).batch(TEST_BATCH_SIZE)\n",
        "\n"
      ],
      "metadata": {
        "id": "0cFAL-Mj7Gd9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_Classfier(Model):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Classfier, self).__init__()\n",
        "\n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(64, activation= 'relu')\n",
        "        self.d2 = Dense(10, activation= 'softmax')\n",
        "    def call(self,x):\n",
        "        x = self.flatten(x)\n",
        "        x= self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uz1GMn2v7Ghm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_metrics():\n",
        "    global train_loss, train_acc\n",
        "    global validation_loss, validation_acc\n",
        "    global test_loss, test_acc\n",
        "\n",
        "    train_loss = Mean()\n",
        "    validation_loss = Mean()\n",
        "    test_loss = Mean()\n",
        "\n",
        "    train_acc = SparseCategoricalAccuracy()\n",
        "    validation_acc = SparseCategoricalAccuracy()\n",
        "    test_acc = SparseCategoricalAccuracy()"
      ],
      "metadata": {
        "id": "fO0E5wq98a9B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def trainer():\n",
        "    global train_ds, model, loss_object, optimizer\n",
        "    global train_loss, train_acc\n",
        "    for images, labels in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(images)\n",
        "            loss = loss_object(labels, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_acc(labels, predictions)"
      ],
      "metadata": {
        "id": "2Fa1E9mn8uUr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def validation():\n",
        "    global validation_ds, model, loss_object\n",
        "    global validation_loss, validation_acc\n",
        "    for images, labels in validation_ds:\n",
        "        predictions = model(images)\n",
        "        loss = loss_object(labels, predictions)\n",
        "\n",
        "        validation_loss(loss)\n",
        "        validation_acc(labels, predictions)"
      ],
      "metadata": {
        "id": "AKqM-zrL9a6W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def tester():\n",
        "    global test_ds, model, loss_object\n",
        "    global test_loss, test_acc\n",
        "    for images, labels in test_ds:\n",
        "        predictions = model(images)\n",
        "        loss = loss_object(labels, predictions)\n",
        "\n",
        "        test_loss(loss)\n",
        "        test_acc(labels, predictions)\n",
        "'''\n",
        "    template ='test Loss: {:.4f}\\t test accuracy:{:.2f}%\\n'\n",
        "\n",
        "    print(template.format(test_loss.result(),\n",
        "                          test_acc.result() * 100))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4LcqtNLL9pWl",
        "outputId": "944bc84c-84cc-49b3-c84e-1e802a1a86d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n    template ='test Loss: {:.4f}\\t test accuracy:{:.2f}%\\n'\\n\\n    print(template.format(test_loss.result(),\\n                          test_acc.result() * 100))\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reporter():\n",
        "    global epoch\n",
        "    global train_loss, train_acc\n",
        "    global validation_loss, validation_acc\n",
        "    #on \n",
        "    print(colored('Epoch','red','on_white'), epoch+1)\n",
        "\n",
        "    template = 'Train Loss: {:.4f}\\t Train Accuracy:{:.2f}%\\n' +\\\n",
        "    'validation Loss: {:.4f}\\t valiation accuracy:{:.2f}%\\n'\n",
        "\n",
        "    print(template.format(train_loss.result(),\n",
        "                          train_acc.result() * 100,\n",
        "                          validation_loss.result(),\n",
        "                          validation_acc.result() * 100))\n",
        "    train_acc.reset_states()\n",
        "    train_loss.reset_states()\n",
        "    validation_loss.reset_states()\n",
        "    validation_acc.reset_states()"
      ],
      "metadata": {
        "id": "6urWVid6_a2d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "TEST_BATCH_SIZE = 32\n",
        "\n",
        "standardization(TRAIN_BATCH_SIZE,TEST_BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "waLf0vnL9yD8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNIST_Classfier()\n",
        "loss_object = SparseCategoricalCrossentropy()\n",
        "optimizer = SGD(learning_rate=LR)\n",
        "\n",
        "load_metrics()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    trainer()\n",
        "    validation()\n",
        "    train_reporter()\n",
        "\n",
        "tester()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A9SaRKq-XAg",
        "outputId": "cec35842-4da6-4cc6-8538-72300c692dde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[47m\u001b[31mEpoch\u001b[0m 1\n",
            "Train Loss: 1.4219\t Train Accuracy:66.02%\n",
            "validation Loss: 0.8731\t valiation accuracy:81.85%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 2\n",
            "Train Loss: 0.6993\t Train Accuracy:84.25%\n",
            "validation Loss: 0.5879\t valiation accuracy:85.82%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 3\n",
            "Train Loss: 0.5314\t Train Accuracy:86.72%\n",
            "validation Loss: 0.4888\t valiation accuracy:87.52%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 4\n",
            "Train Loss: 0.4597\t Train Accuracy:88.00%\n",
            "validation Loss: 0.4377\t valiation accuracy:88.52%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 5\n",
            "Train Loss: 0.4190\t Train Accuracy:88.73%\n",
            "validation Loss: 0.4057\t valiation accuracy:89.12%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 6\n",
            "Train Loss: 0.3922\t Train Accuracy:89.29%\n",
            "validation Loss: 0.3841\t valiation accuracy:89.59%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 7\n",
            "Train Loss: 0.3728\t Train Accuracy:89.71%\n",
            "validation Loss: 0.3685\t valiation accuracy:89.97%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 8\n",
            "Train Loss: 0.3579\t Train Accuracy:90.02%\n",
            "validation Loss: 0.3557\t valiation accuracy:90.34%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 9\n",
            "Train Loss: 0.3458\t Train Accuracy:90.35%\n",
            "validation Loss: 0.3464\t valiation accuracy:90.55%\n",
            "\n",
            "\u001b[47m\u001b[31mEpoch\u001b[0m 10\n",
            "Train Loss: 0.3358\t Train Accuracy:90.61%\n",
            "validation Loss: 0.3372\t valiation accuracy:90.80%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dGOqntyO-von"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}